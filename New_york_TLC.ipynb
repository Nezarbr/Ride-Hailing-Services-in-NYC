{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import folium\n",
    "from shapely import wkt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore') # Ignore all warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Price_waiting_processed.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_price \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPrice_waiting_processed.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m# dataframe for the trip fare prediction\u001b[39;00m\n\u001b[1;32m      3\u001b[0m df_demand \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDemand_forecast.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;66;03m# dataframe for the demand forecast\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Price_waiting_processed.csv'"
     ]
    }
   ],
   "source": [
    "df_price = pd.read_csv('Price_waiting_processed.csv')# dataframe for the trip fare prediction\n",
    "\n",
    "df_demand = pd.read_csv('Demand_forecast.csv')# dataframe for the demand forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General analysis\n",
    "\n",
    "df_price.drop(columns=['airport_fee','tolls','wav_match_flag','wav_request_flag','access_a_ride_flag','tips','congestion_surcharge','shared_match_flag','shared_request_flag'], inplace=True) \n",
    "# Explanation: Drops columns with a high percentage of NaN values or non-informative data, such as 'airport_fee' with 99% NaN values, 'tolls' where 90% of the data is 0,\n",
    "#  'congestion_surcharge' with 60% zeros, 'shared_match_flag' and 'shared_request_flag' with the same value across the dataset, 'tips' with 80% zeros,\n",
    "#  'access_a_ride_flag' with an undetermined value, 'wav_request_flag' with 90% being 'No', and 'wav_match_flag'\n",
    "#  on_scene_datetime : nan values for Lyft and Via about 90% of their records (same thing for originating_base_num)\n",
    "\n",
    "df_price = df_price.dropna(subset=['request_datetime']).reset_index(drop=True) \n",
    "# Explanation: Removes records with NaN values in 'request_datetime', resulting in the removal of 4 records.\n",
    "\n",
    "conditions = [\n",
    "    (df_price['driver_pay'] > 5) & (df_price['driver_pay'] < 60), # 2% of the records are not adapted, negative values and outliers\n",
    "    (df_price['trip_miles'] > 0.5) & (df_price['trip_miles'] < 30), # 99% of the records are below 30 miles\n",
    "    (df_price['trip_time'] > 180) & (df_price['trip_time'] < 3500), # 98% of the records are within the specified range; the remaining 2% are considered outliers\n",
    "    (df_price['base_passenger_fare'] > 5) & (df_price['base_passenger_fare'] < 70), # 98% of the records fall within this range; the rest are outliers\n",
    "    (df_price['sales_tax'] < 6) # 99.5% of the records are within this range\n",
    "]\n",
    "\n",
    "mask = np.all(conditions, axis=0) # Apply the conditions\n",
    "\n",
    "df_price = df_price[mask].reset_index(drop=True) # Filter the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction for fare-related analysis\n",
    "\n",
    "# Replace the license numbers with company names\n",
    "replace_dict = {'HV0003': 'Uber', 'HV0005': 'Lyft', 'HV0004': 'Via'}\n",
    "df_price['hvfhs_license_num'] = df_price['hvfhs_license_num'].map(replace_dict).fillna(df_price['hvfhs_license_num'])\n",
    "\n",
    "# Time features extraction:\n",
    "df_price['request_datetime'] = pd.to_datetime(df_price['request_datetime'])\n",
    "df_price['pickup_datetime'] = pd.to_datetime(df_price['pickup_datetime'])\n",
    "\n",
    "df_price['hour_request'] = df_price['request_datetime'].dt.hour\n",
    "df_price['week_day_request'] = df_price['request_datetime'].dt.dayofweek\n",
    "df_price['day_category'] = df_price['week_day_request'].apply(lambda x: 'Weekend' if x >= 4 else 'Weekday') # Categorizes days into 'Weekend' or 'Weekday' based on the day of the week.\n",
    "\n",
    "df_price['waiting_time'] = (df_price['pickup_datetime'] - df_price['request_datetime']).dt.total_seconds() # Feature extraction: Calculate the waiting time in seconds\n",
    "\n",
    "df_price = df_price[(df_price['waiting_time'] > 90) & (df_price['waiting_time'] < 500)].reset_index(drop=True) # Filter out records where the waiting time is not within the specified range (90 to 500 seconds), which accounts for about 5% of the records.\n",
    "\n",
    "########\n",
    "\n",
    "df_price['fare_price'] = df_price['bcf'] + df_price['base_passenger_fare'] + df_price['sales_tax'] # Explanation: 'fare_price' feature that includes all the costs.\n",
    "\n",
    "df_price['ratio_Price_miles'] = df_price['fare_price'] / df_price['trip_miles'] # Explanation: Creates a new feature 'ratio_Price_miles', representing the fare cost per mile.\n",
    "\n",
    "df_price = df_price[(df_price['ratio_Price_miles'] < 20) & (df_price['ratio_Price_miles'] > 1)].reset_index(drop=True)\n",
    "# Explanation: Filters out records with unrealistic 'ratio_Price_miles' values (either too high or too low), likely due to outliers or data entry errors (2%).\n",
    "\n",
    "# Add a new feature that represents the zone pairs\n",
    "df_price['PULocationID'] = df_price['PULocationID'].astype(str)\n",
    "df_price['DOLocationID'] = df_price['DOLocationID'].astype(str)\n",
    "df_price['Zones_pairs'] = df_price['PULocationID'] + df_price['DOLocationID']\n",
    "\n",
    "\n",
    "###### to Avoid leakage\n",
    "\n",
    "train_df, test_df = train_test_split(df_price, test_size=0.25, random_state=42) # Splitting the dataframe into training and test sets\n",
    "\n",
    "\n",
    "mean_ratio_per_location_end_train = train_df.groupby('DOLocationID')['ratio_Price_miles'].mean() # Calculate the mean ratio of price per mile for each end location in the training set\n",
    "\n",
    "train_df['ratio_end_location'] = train_df['DOLocationID'].map(mean_ratio_per_location_end_train) # Map the calculated mean ratio from the training set to both the training and test sets\n",
    "test_df['ratio_end_location'] = test_df['DOLocationID'].map(mean_ratio_per_location_end_train).fillna(0)  # Using 0 or an appropriate value for missing entries\n",
    "\n",
    "mean_ratio_per_location_start_train = train_df.groupby('PULocationID')['ratio_Price_miles'].mean() # Calculate the mean ratio of price per mile for each start location in the training set\n",
    "\n",
    "train_df['ratio_Dep_location'] = train_df['PULocationID'].map(mean_ratio_per_location_start_train) # Map the calculated mean ratio from the training set to both the training and test sets\n",
    "test_df['ratio_Dep_location'] = test_df['PULocationID'].map(mean_ratio_per_location_start_train).fillna(0)  # Using 0 or an appropriate value for missing entries\n",
    "\n",
    "mean_ratio_per_zonespair_train = train_df.groupby('Zones_pairs')['ratio_Price_miles'].mean() # Calculate the mean ratio of price per mile for each zone pair in the training set\n",
    "\n",
    "train_df['ratio_zones_pair'] = train_df['Zones_pairs'].map(mean_ratio_per_zonespair_train) # Map the calculated mean ratio from the training set to both the training and test sets\n",
    "test_df['ratio_zones_pair'] = test_df['Zones_pairs'].map(mean_ratio_per_zonespair_train).fillna(0)  # Using 0 or an appropriate value for missing entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction for Demand-related analysis\n",
    "\n",
    "df_demand = df_demand.groupby('date_request_h')['Number_fares'].sum().reset_index()\n",
    "\n",
    "df_demand['date_request_h'] = pd.to_datetime(df_demand['date_request_h'])\n",
    "df_demand['day_of_week'] = df_demand['date_request_h'].dt.dayofweek # Extract the day of the week\n",
    "df_demand['hour'] = df_demand['date_request_h'].dt.hour # Extract the hour\n",
    "df_demand['day_of_month'] = df_demand['date_request_h'].dt.day # Extract the day of the month\n",
    "df_demand['month'] = df_demand['date_request_h'].dt.month  # Extracts the month\n",
    "\n",
    "df_demand = df_demand.set_index('date_request_h') \n",
    "\n",
    "target_M = df_demand['Number_fares'].to_dict() \n",
    "\n",
    "# Creating lag features to capture temporal dependencies in the data\n",
    "for i in range(1, 22):\n",
    "    # For each day from 1 to 21, create a new column 'lagX' where X is the lag day count.\n",
    "    # This calculates the demand lag .\n",
    "    df_demand['lag' + str(i)] = (df_demand.index - pd.Timedelta(str(1*i) + ' days')).map(target_M)\n",
    "\n",
    "# Drop any rows that have NaN values in any of the lag feature columns\n",
    "df_demand = df_demand.dropna(subset=['lag' + str(i) for i in range(1, 22)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter DataFrames for each service\n",
    "uber_df = train_df[train_df['hvfhs_license_num'] == 'Uber']\n",
    "lyft_df = train_df[train_df['hvfhs_license_num'] == 'Lyft']\n",
    "via_df = train_df[train_df['hvfhs_license_num'] == 'Via']\n",
    "\n",
    "\n",
    "def mean_price_and_driver_pay_per_bin_adjusted(dataframe, column='trip_miles', \n",
    "                                                price_column='fare_price', pay_column='driver_pay', bins=20):\n",
    "    _, bin_edges = pd.cut(dataframe[column], bins=bins, retbins=True)# Define bins\n",
    "    dataframe['bin'] = pd.cut(dataframe[column], bins=bin_edges) # Create a new column for bins\n",
    "    mean_prices = dataframe.groupby('bin')[price_column].mean() # Calculate mean price per bin\n",
    "    mean_pay = dataframe.groupby('bin')[pay_column].mean()  # Calculate mean driver pay per bin\n",
    "    \n",
    "    bin_mids = (bin_edges[:-1] + bin_edges[1:]) / 2  # Calculate midpoints of bins using the bin edges\n",
    "    \n",
    "    return bin_mids, mean_prices.values, mean_pay.values\n",
    "\n",
    "# Recalculate mean prices and driver pay per bin using the adjusted function for each company\n",
    "uber_bin_mids, uber_means, uber_driver_pay = mean_price_and_driver_pay_per_bin_adjusted(uber_df, 'trip_miles', 'fare_price', 'driver_pay')\n",
    "lyft_bin_mids, lyft_means, lyft_driver_pay = mean_price_and_driver_pay_per_bin_adjusted(lyft_df, 'trip_miles', 'fare_price', 'driver_pay')\n",
    "via_bin_mids, via_means, via_driver_pay = mean_price_and_driver_pay_per_bin_adjusted(via_df, 'trip_miles', 'fare_price', 'driver_pay')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot fare price data\n",
    "plt.plot(uber_bin_mids, uber_means, label='Uber Fare Price', marker='o', color='blue')\n",
    "plt.plot(lyft_bin_mids, lyft_means, label='Lyft Fare Price', marker='o', color='green')\n",
    "plt.plot(via_bin_mids, via_means, label='Via Fare Price', marker='o', color='red')\n",
    "\n",
    "# Plot driver pay data\n",
    "plt.plot(uber_bin_mids, uber_driver_pay, label='Uber Driver Pay', linestyle='--', color='blue')\n",
    "plt.plot(lyft_bin_mids, lyft_driver_pay, label='Lyft Driver Pay', linestyle='--', color='green')\n",
    "plt.plot(via_bin_mids, via_driver_pay, label='Via Driver Pay', linestyle='--', color='red')\n",
    "\n",
    "plt.xlabel('Trip Distance (Midpoint of Bins)')\n",
    "plt.ylabel('Mean Price/Pay')\n",
    "#plt.title('Mean Fare Price and Driver Pay per Trip Distance Bin by Company')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_waiting_time_per_bin(dataframe, column='trip_miles', waiting_time_column='waiting_time', bins=20):\n",
    "    _, bin_edges = pd.cut(dataframe[column], bins=bins, retbins=True)\n",
    "    \n",
    "    dataframe['bin'] = pd.cut(dataframe[column], bins=bin_edges)\n",
    "    mean_waiting_times = dataframe.groupby('bin')[waiting_time_column].mean()\n",
    "    bin_mids = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    \n",
    "    return bin_mids, mean_waiting_times.values\n",
    "\n",
    "\n",
    "# Recalculate mean waiting times per bin for each company\n",
    "uber_bin_mids, uber_means = mean_waiting_time_per_bin(uber_df, 'trip_miles', 'waiting_time')\n",
    "lyft_bin_mids, lyft_means = mean_waiting_time_per_bin(lyft_df, 'trip_miles', 'waiting_time')\n",
    "via_bin_mids, via_means = mean_waiting_time_per_bin(via_df, 'trip_miles', 'waiting_time')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(uber_bin_mids, uber_means, label='Uber', marker='o')\n",
    "plt.plot(lyft_bin_mids, lyft_means, label='Lyft', marker='o')\n",
    "plt.plot(via_bin_mids, via_means, label='Via', marker='o')\n",
    "\n",
    "plt.xlabel('Trip Distance (Midpoint of Bins)')\n",
    "plt.ylabel('Mean Waiting Time (Seconds)')\n",
    "#plt.title('Mean Waiting Time per Trip Distance Bin by Company')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cdf(data, label):\n",
    "    x = np.sort(data)\n",
    "    y = np.arange(1, len(x)+1) / len(x)\n",
    "    # Plotting the CDF\n",
    "    plt.plot(x, y, label=label)\n",
    "\n",
    "# Plotting CDF for both 'ratio_end_location' and 'ratio_Dep_location'\n",
    "plot_cdf(train_df['ratio_end_location'], 'End Location Ratio')\n",
    "plot_cdf(train_df['ratio_Dep_location'], 'Departure Location Ratio')\n",
    "plot_cdf(train_df['ratio_zones_pair'], 'Zones Pair Ratio')\n",
    "\n",
    "plt.xlabel('Ratio Price per Miles')\n",
    "plt.ylabel('CDF')\n",
    "plt.title('Cumulative Distribution Function of Ratios')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pdf(data, label, bins=30, density=True, alpha=0.5):\n",
    "    # Plotting the PDF using a histogram\n",
    "    plt.hist(data, bins=bins, density=density, alpha=alpha, label=label)\n",
    "\n",
    "\n",
    "# Plotting PDF for 'ratio_end_location', 'ratio_Dep_location', and 'ratio_zones_pair'\n",
    "plot_pdf(train_df['ratio_end_location'], 'End Location Ratio')\n",
    "plot_pdf(train_df['ratio_Dep_location'], 'Departure Location Ratio')\n",
    "plot_pdf(train_df['ratio_zones_pair'], 'Zones Pair Ratio')\n",
    "\n",
    "plt.xlabel('Ratio Price per Miles')\n",
    "plt.ylabel('Probability Density')\n",
    "#plt.title('Probability Density Function of Ratios')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_records = train_df.groupby(['hour_request', 'day_category']).size().unstack(fill_value=0)\n",
    "\n",
    "mean_price_per_hour = train_df.groupby(['hour_request', 'day_category'])['fare_price'].mean().unstack(fill_value=0) # Calculate mean fare_price per hour for each day category\n",
    "\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "hours = mean_price_per_hour.index.values  \n",
    "width = 0.35  \n",
    "\n",
    "# Bar plot for the number of records\n",
    "ax1.bar(hours - width/2, hourly_records['Weekday'].values, width, label='Records - Weekday', color='tab:gray')\n",
    "ax1.bar(hours + width/2, hourly_records['Weekend'].values, width, label='Records - Weekend', color='tab:orange')\n",
    "\n",
    "ax1.set_xlabel('Hour of Request')\n",
    "ax1.set_ylabel('Number of Records', color='tab:gray')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:gray')\n",
    "ax1.set_xticks(hours)\n",
    "ax1.set_xticklabels(hours.astype(str), rotation=45)  \n",
    "\n",
    "# Create a twin Axes sharing the x-axis for mean fare price line plots\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(hours, mean_price_per_hour['Weekday'].values, label='Mean Fare Price - Weekday', color='tab:blue', marker='o', linestyle='-')\n",
    "ax2.plot(hours, mean_price_per_hour['Weekend'].values, label='Mean Fare Price - Weekend', color='tab:cyan', marker='o', linestyle='-')\n",
    "\n",
    "ax2.set_ylabel('Mean Fare Price', color='tab:blue')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc='upper left')\n",
    "\n",
    "plt.title('Hourly Trends: Number of Records and Mean Fare Price')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'date_request_h' in df_demand.columns: # Check if 'date_request_h' is the index, if not, set it as the index\n",
    "    df_demand.set_index('date_request_h', inplace=True)\n",
    "\n",
    "df_demand.index = pd.to_datetime(df_demand.index) # Ensure the index is in datetime format\n",
    "\n",
    "df_demand['Number_fares'].plot(figsize=(15, 7), marker='o', linestyle='-', markersize=2)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Requests')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = adfuller(df_demand['Number_fares']) # Perform Augmented Dickey-Fuller test\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print('\\t%s: %.3f' % (key, value))\n",
    "\n",
    "# Interpretation\n",
    "if result[1] > 0.05:\n",
    "    print(\"Series is non-stationary\")\n",
    "else:\n",
    "    print(\"Series is stationary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding for 'hvfhs_license_num'\n",
    "X_train = pd.get_dummies(train_df[['trip_miles','hour_request','month','week_day_request','hvfhs_license_num','ratio_zones_pair','ratio_Dep_location','ratio_end_location']])\n",
    "y_train = train_df['fare_price']\n",
    "\n",
    "# Split the data into training and testing sets (75% train, 25% test)\n",
    "X_test = pd.get_dummies(test_df[['trip_miles','hour_request','month','week_day_request','hvfhs_license_num','ratio_zones_pair','ratio_Dep_location','ratio_end_location']])\n",
    "y_test = test_df['fare_price']\n",
    "\n",
    "\n",
    "max_depth = 15  # Specify the depth directly\n",
    "\n",
    "model = DecisionTreeRegressor(max_depth=max_depth, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R^2 Score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - y_pred\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "absolute_residuals = np.abs(residuals)\n",
    "sorted_abs_residuals = np.sort(absolute_residuals)\n",
    "cumulative_proportion = np.linspace(0, 1, len(sorted_abs_residuals), endpoint=False)\n",
    "plt.plot(sorted_abs_residuals, cumulative_proportion)\n",
    "plt.xlabel('Absolute Residuals')\n",
    "plt.ylabel('Cumulative Proportion')\n",
    "#plt.title('CDF of Absolute Residuals')\n",
    "plt.grid(True)\n",
    "\n",
    "idx = np.where(cumulative_proportion > 0.8)[0][0]# Find the index where the cumulative proportion surpasses 0.8\n",
    "\n",
    "plt.axvline(x=sorted_abs_residuals[idx], color='r', linestyle='--', label=f'Threshold at {sorted_abs_residuals[idx]:.2f}')# Add a vertical line at x-value when cumulative proportion surpasses 0.8\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure demonstrates that approximately 80\\% of the predictions have an error margin between -5.19 and 5.19, highlighting the model's precision in estimating fare prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = model.feature_importances_\n",
    "\n",
    "sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "\n",
    "labels = [X_train.columns[i] for i in sorted_idx]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.bar(range(X_train.shape[1]), feature_importance[sorted_idx], align='center')\n",
    "plt.xticks(range(X_train.shape[1]), labels, rotation='vertical')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Feature Importance')\n",
    "plt.title('Feature Importance in Decision Tree Model')\n",
    "\n",
    "# Add percentages inside the bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height, '{:.2f}%'.format(height * 100),\n",
    "             ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_demand.loc[df_demand.index < '2021-05-24 00:00:00']\n",
    "test = df_demand.loc[df_demand.index >= '2021-05-24 00:00:00']\n",
    "\n",
    "Features = ['day_of_week','day_of_month','hour','lag7','lag14','lag21','month']\n",
    "Target = 'Number_fares'\n",
    "\n",
    "X_train = train[Features]\n",
    "y_train = train[Target]\n",
    "\n",
    "X_test = test[Features]\n",
    "y_test = test[Target]\n",
    "\n",
    "reg = xgb.XGBRegressor(n_estimators=10000, \n",
    "                       learning_rate=0.001,\n",
    "                       random_state=42)\n",
    "\n",
    "reg.fit(X_train, y_train, \n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)], \n",
    "        early_stopping_rounds=50, \n",
    "        verbose=100)\n",
    "\n",
    "results = reg.evals_result()\n",
    "\n",
    "train_rmse = results['validation_0']['rmse']\n",
    "val_rmse = results['validation_1']['rmse']\n",
    "iterations = range(1, len(train_rmse) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(iterations, train_rmse, label='Train RMSE')\n",
    "plt.plot(iterations, val_rmse, label='Validation RMSE')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Training and Validation RMSE over Iterations')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred = reg.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=y_test, y=y_pred)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure illustrates that the predicted values closely align with the actual data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Zones classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price['waiting_time'] = (pd.to_datetime(df_price['pickup_datetime']) - pd.to_datetime(df_price['request_datetime'])).dt.total_seconds() / 60.0  # in minutes\n",
    "\n",
    "pickup_stats = df_price.groupby('PULocationID').agg(\n",
    "    num_trips_per_PULocationID=('PULocationID', 'size'),\n",
    "    mean_waiting_time_per_PULocationID=('waiting_time', 'mean'),\n",
    "    mean_ratio_Price_miles_per_PULocationID=('ratio_Price_miles', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "dropoff_stats = df_price.groupby('DOLocationID').agg(\n",
    "    num_trips_per_DOLocationID=('DOLocationID', 'size')\n",
    ").reset_index()\n",
    "\n",
    "pickup_stats.rename(columns={'PULocationID': 'LocationID'}, inplace=True)\n",
    "dropoff_stats.rename(columns={'DOLocationID': 'LocationID'}, inplace=True)\n",
    "\n",
    "location_stats = pd.merge(pickup_stats, dropoff_stats, on='LocationID', how='outer')\n",
    "\n",
    "location_stats['num_trips_per_PULocationID'].fillna(0, inplace=True)\n",
    "location_stats['num_trips_per_DOLocationID'].fillna(0, inplace=True)\n",
    "\n",
    "location_stats = location_stats.dropna().reset_index(drop=True)\n",
    "location_stats['LocationID'] = location_stats['LocationID'].astype(int)\n",
    "location_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='mean')\n",
    "location_stats_filled = imputer.fit_transform(location_stats[['num_trips_per_PULocationID', 'num_trips_per_DOLocationID', 'mean_waiting_time_per_PULocationID', 'mean_ratio_Price_miles_per_PULocationID']])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "location_stats_scaled = scaler.fit_transform(location_stats_filled)\n",
    "\n",
    "# Choosing the Number of Clusters (K) - Elbow Method\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=42)\n",
    "    kmeans.fit(location_stats_scaled)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plotting the results onto a line graph to observe the 'elbow'\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(range(1, 11), wcss, marker='o', linestyle='--')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying K-Means Clustering\n",
    "\n",
    "n_clusters = 4  # Example based on visual inspection of the elbow plot\n",
    "kmeans = KMeans(n_clusters=n_clusters, init='k-means++', max_iter=300, n_init=10, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(location_stats_scaled)\n",
    "\n",
    "location_stats['Cluster'] = cluster_labels\n",
    "\n",
    "location_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "feature_names = ['num_trips_per_PULocationID', 'num_trips_per_DOLocationID', 'mean_waiting_time_per_PULocationID', 'mean_ratio_Price_miles_per_PULocationID']\n",
    "centroids_df = pd.DataFrame(centroids, columns=feature_names)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for feature in feature_names:\n",
    "    plt.plot(centroids_df[feature], label=feature)\n",
    "\n",
    "plt.xticks(ticks=np.arange(n_clusters), labels=[f'Cluster {i}' for i in range(1,n_clusters+1)])\n",
    "plt.ylabel('Centroid Coordinate')\n",
    "plt.title('Centroid Coordinates by Feature')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones = pd.read_csv('nyc-taxi-zones-1.csv')\n",
    "\n",
    "merged_df = pd.merge(zones, location_stats, on='LocationID', how='inner')\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 1: Represents areas with slightly lower demand, shorter waiting times, and lower prices per mile (represented in green on the map below).\n",
    "\n",
    "Cluster 2: Corresponds to areas with cheaper fares and lower demand for taxi services (represented in blue on the map below).\n",
    "\n",
    "Cluster 3: Indicates areas with high transportation demand, suggesting busy routes and potentially higher fares (represented in red on the map below).\n",
    "\n",
    "Cluster 4: Represents areas with slightly higher demand and price per mile, along with shorter waiting times. Likely experiencing moderate transportation demand with slightly higher fares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_stats['Cluster'] = location_stats['Cluster'].astype(int)\n",
    "merged_df = pd.merge(zones, location_stats, how='inner', on='LocationID')\n",
    "\n",
    "# Define a distinct color map for clusters\n",
    "cluster_colors = {\n",
    "    0: 'green',\n",
    "    1: 'blue',\n",
    "    2: 'red',\n",
    "    3: 'orange',\n",
    "}\n",
    "\n",
    "# Map colors in the DataFrame\n",
    "merged_df['Color'] = merged_df['Cluster'].map(cluster_colors).fillna('gray')\n",
    "\n",
    "# Center of the map - New York's geographic center approximation\n",
    "map_center = [40.7128, -74.0060]\n",
    "folium_map = folium.Map(location=map_center, zoom_start=10)\n",
    "\n",
    "for _, row in merged_df.iterrows():\n",
    "    polygon = wkt.loads(row['the_geom'])\n",
    "    simple_polygon = polygon.simplify(tolerance=0.001, preserve_topology=True)\n",
    "    tooltip_text = f\"Borough: {row['borough']} <br> Zone name: {row['zone']}\"\n",
    "    \n",
    "    geojson = folium.GeoJson(\n",
    "        data=simple_polygon,\n",
    "        style_function=lambda x, color=row['Color']: {\n",
    "            'fillColor': color, \n",
    "            'color': color, \n",
    "            'weight': 1, \n",
    "            'fillOpacity': 0.3\n",
    "        },\n",
    "        tooltip=tooltip_text\n",
    "    )\n",
    "    \n",
    "    geojson.add_to(folium_map)\n",
    "\n",
    "\n",
    "folium_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
